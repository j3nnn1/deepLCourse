Apuntes de tutoria 20210321

* numpy array estan hechos para modelar vectores, matrices y tensores.
* Las listas te permiten  que las componentes de la lista sean heterogeneas.
* datos sparsos: matrices vectores donde la mayoria de sus componentes son 0.
* numpy array reserva espacio a priori para los componentes del vector o matrix.
* bag of words (unA forma de tantas de representar oraciones): que conviene elegir? numpy array o listas?  aca construye un vector donde el len del vector es igual al tama;o del vocabulario. un vocabulario chico son 10000 palabras y uno grande esta por 1000000 de palabras.   

Esta Matrix tiene un sparcity del 98% la mayoria de los elementos son 0.
numpy tiene un tipo de elementos llamado sparse.matrix que mantiene mejor el uso de la memoria, solo guarda lo que tienen el 2% con sus valores.

| v | = vector que contiene bag of word.

L = lista que contiene el vocabulario del modelo.

Guarda de esta forma la matrix:
fila, columna = valor

		| V |
# articulos    

* La definicion del vocabulario lo puedes guardar en una lista. 

* herramienta deep speach??? open source. https://github.com/mozilla/DeepSpeech  Para hacer speach to teach recomienda la api de google como alternativa.

* entrenar una hmm (hidden markov model)? o una red neuronales recurrente (Bidireccional RNN)? pero son costosas las redes neuronales por lo que siempre te conviene usar el hmm (https://jonathan-hui.medium.com/machine-learning-hidden-markov-model-hmm-31660d217a61)

* Modelos => Encoder => genera un vector de baja dimensionalidad -> Decoder -> X prima.

Ejemplos que se resuelven con esta arquitectura:

el problema de traduccion se ajusta a esto.
X oraciones en idioma 1 => X prima puede ser la oracion en otro idioma.
X puede ser una oracion => X prima un texto que describe la imagen.
X puede ser un audio =>  X prima puede ser un texto. (viceversa)


Para imagenes vamos a ver redes convusionales CNN o algo como transfer learning. agarrar redes preentrenadas para ajustarlas un poco para que terminen de entender el nuevo problema, a diferencia de las RNN es muy dificiil usar transfer learning, hoy en dia esta surgiendo un nuevo paradigma "Transform learning" para sustitur las RNN ya que son dificil de reutilizar una vez entrenadas.
HMM(hidden markov model) no lo vamos a ver.



Monty Hall

* Numpy array con 1000 con la puerta elegida
* Numpy array con 1000 si cambio o no cambio.
* Numpy array con 1000 .
* Numpy operaciones vectorizadas con numpy array puedes paralelizar operaciones.
* lista de puertas elegidas.
* operacion de las decisiones
* el lab de monty hall fue pensado para que usaramos un FOR.
* probar agregar o resolver problemas de manera vectorizada. sin IF ni FOR

Preguntas de los examenes del campus.

Propiedades de la gausiana bivariada.

matrix de covarianza sumatoria de 1 a N

Existen distribuciones NO parametricas.

Ejemplo el histograma.


Esto se le llama distribucion parametrica.

Con cuantos parametros queda definida una distribucion gausiana univariada.
    - media
    - varianza o desviacion estandar.

X ~ N (mu, sigma)

revisar github lab-pep-itba/ intro probabilidad. / distribuciones.

La forma de la distribucion gausiana esta definida por

(Constante E : altura total o el factor pro el cual tengo que multiplicar para que la integral de 1)

f (v) = Constante E ^  - (v -m) ^ 2  (media)/ 2 sigma ^2 (desvio estandar)

tiende a E a la - infinito que es 0.

Si sigma es muy chico la distribucion decrece de forma muy grande.

(v -m): medida de similitud entre mu y V o funcion de base radial (RBF) para que la integral de 1.

En distribuciones Continuas univariada.

media => integral de - infinito a + infinito de f(v) d(v) = 1

Covarianza => (v - MU )^2 * f (v) dv = sigma ^ cuadrado


En distribuciones Continuas  multivariadas.

Funcion de base radial (mide la distancia entre el punto X al vector MU)
depdende de la distancia con respecto al radio.


Covarianza indica a que velocidad varia a medida que me alejo del centro.

a 0
0 a

si A es mas grande la campana va a ser mas grande y si es mas chico la campana va a estar ajustada

a 0
0 b

si a es mas grande que b significa que en X =1

indenpendencia y correlacion en la gauseana es lo mismo.

a d
d b

si d > 0 valores positivos de x1 hacen que uno espere valores mas grande de x2.


si d < 0 se invierte la diagonal.















